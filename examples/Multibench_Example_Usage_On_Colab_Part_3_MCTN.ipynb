{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multibench Example Usage On Colab Part 3: MCTNipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome!\n",
        "\n",
        "This example shows a slightly more complicated training paradigm in MultiBench. Namely, we'll run L2-MCTN on MOSI.\n",
        "\n",
        "This tutorial assumes you've followed along with the first tutorial, as we'll focus on the differences between this task and standard supervised learning in MultiBench.\n",
        "\n",
        "To begin, let's clone the repo and setup our interpreter to run commands inside the folder."
      ],
      "metadata": {
        "id": "JCnG1gTFJQ-4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHmaOz8aEZx6",
        "outputId": "c01ae7db-3421-4f0a-dba6-1e0b1e85563a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MultiBench'...\n",
            "remote: Enumerating objects: 4890, done.\u001b[K\n",
            "remote: Counting objects: 100% (1906/1906), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1018/1018), done.\u001b[K\n",
            "remote: Total 4890 (delta 1289), reused 1370 (delta 885), pack-reused 2984\u001b[K\n",
            "Receiving objects: 100% (4890/4890), 46.51 MiB | 32.22 MiB/s, done.\n",
            "Resolving deltas: 100% (3347/3347), done.\n",
            "/content/MultiBench/MultiBench\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pliang279/MultiBench.git\n",
        "%cd MultiBench"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to download the data file for MOSI using the below command. If this does not work for you, please download the data file locally, and upload it to the folder \"/content/MultiBench/\""
      ],
      "metadata": {
        "id": "tUqFe87DIYu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!mkdir temp\n",
        "!pip install gdown && gdown https://drive.google.com/u/0/uc?id=1szKIqO0t3Be_W91xvf6aYmsVVUa7wDHU&export=download"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwZS6dfGElh8",
        "outputId": "0c857832-d377-4134-aeab-91016225f708"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.63.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/u/0/uc?id=1KvKynJJca5tDtI5Mmp6CoRh9pQywH8Xp \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As Colab famously has bad handling of Conda env files, we'll install the dependencies manually so that it works. Please note that other systems might require installation of a long list of other dependencies."
      ],
      "metadata": {
        "id": "nd1ZaCe6JOoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install memory-profiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSn8hNwXOAIh",
        "outputId": "9cde8d78-b36b-4bdb-c968-cd3ade910b18"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting memory-profiler\n",
            "  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler) (5.4.8)\n",
            "Building wheels for collected packages: memory-profiler\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=72f96169512ed301798c7fcc28fd951c8a7487425a6db3c1a75674d5d1f9b6df\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n",
            "Successfully built memory-profiler\n",
            "Installing collected packages: memory-profiler\n",
            "Successfully installed memory-profiler-0.60.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, let's import some of MultiBench and get working. First, we'll import what is required from all MultiBench programs:"
      ],
      "metadata": {
        "id": "n5S9YcS9J6yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "import sys\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbxLvcuuV6tc",
        "outputId": "f8e6fc72-cd87-45e1-994b-d2c1ef55a987"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training ---------->>\n",
            "Train Epoch 0, total loss: 3.0611612796783447, regression loss: 1.3306723833084106, embedding loss: 1.730488896369934\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 0, MAE: 1.4072818756103516, Acc1: 0.5747663551401869, Acc2: 0.6119402985074627\n",
            "<------------ Saving Best Model\n",
            "\n",
            "start training ---------->>\n",
            "Train Epoch 1, total loss: 3.0371196269989014, regression loss: 1.3246451616287231, embedding loss: 1.7124744653701782\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 1, MAE: 1.4050803184509277, Acc1: 0.5747663551401869, Acc2: 0.6119402985074627\n",
            "<------------ Saving Best Model\n",
            "\n",
            "start training ---------->>\n",
            "Train Epoch 2, total loss: 3.0315959453582764, regression loss: 1.3075841665267944, embedding loss: 1.724011778831482\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 2, MAE: 1.4035106897354126, Acc1: 0.5747663551401869, Acc2: 0.6119402985074627\n",
            "<------------ Saving Best Model\n",
            "\n",
            "start training ---------->>\n",
            "Train Epoch 3, total loss: 3.0435800552368164, regression loss: 1.3205816745758057, embedding loss: 1.7229983806610107\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 3, MAE: 1.4027416706085205, Acc1: 0.5747663551401869, Acc2: 0.6119402985074627\n",
            "<------------ Saving Best Model\n",
            "\n",
            "start training ---------->>\n",
            "Train Epoch 4, total loss: 3.0338621139526367, regression loss: 1.304721474647522, embedding loss: 1.7291406393051147\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 4, MAE: 1.4036531448364258, Acc1: 0.5747663551401869, Acc2: 0.6119402985074627\n",
            "start training ---------->>\n",
            "Train Epoch 5, total loss: 3.0189459323883057, regression loss: 1.312135934829712, embedding loss: 1.7068099975585938\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 5, MAE: 1.4033385515213013, Acc1: 0.5747663551401869, Acc2: 0.6119402985074627\n",
            "start training ---------->>\n",
            "Train Epoch 6, total loss: 3.0253396034240723, regression loss: 1.3133291006088257, embedding loss: 1.7120105028152466\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 6, MAE: 1.4038290977478027, Acc1: 0.5747663551401869, Acc2: 0.6119402985074627\n",
            "start training ---------->>\n",
            "Train Epoch 7, total loss: 3.0168802738189697, regression loss: 1.3176727294921875, embedding loss: 1.6992075443267822\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 7, MAE: 1.4030921459197998, Acc1: 0.5747663551401869, Acc2: 0.6119402985074627\n",
            "start training ---------->>\n",
            "Train Epoch 8, total loss: 3.026230812072754, regression loss: 1.3254823684692383, embedding loss: 1.7007484436035156\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 8, MAE: 1.404464602470398, Acc1: 0.5747663551401869, Acc2: 0.6119402985074627\n",
            "start training ---------->>\n",
            "Train Epoch 9, total loss: 3.01007080078125, regression loss: 1.3106378316879272, embedding loss: 1.6994329690933228\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 9, MAE: 1.4032440185546875, Acc1: 0.5747663551401869, Acc2: 0.6119402985074627\n",
            "start training ---------->>\n",
            "Train Epoch 10, total loss: 3.041593313217163, regression loss: 1.3129425048828125, embedding loss: 1.7286508083343506\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 10, MAE: 1.4043539762496948, Acc1: 0.5747663551401869, Acc2: 0.6119402985074627\n",
            "start training ---------->>\n",
            "Train Epoch 11, total loss: 3.0216119289398193, regression loss: 1.3199337720870972, embedding loss: 1.7016781568527222\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 11, MAE: 1.4016263484954834, Acc1: 0.5747663551401869, Acc2: 0.6119402985074627\n",
            "<------------ Saving Best Model\n",
            "\n",
            "start training ---------->>\n",
            "Train Epoch 12, total loss: 3.0048069953918457, regression loss: 1.3017191886901855, embedding loss: 1.7030878067016602\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 12, MAE: 1.4001960754394531, Acc1: 0.5747663551401869, Acc2: 0.6119402985074627\n",
            "<------------ Saving Best Model\n",
            "\n",
            "start training ---------->>\n",
            "Train Epoch 13, total loss: 3.000941038131714, regression loss: 1.2870941162109375, embedding loss: 1.7138469219207764\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 13, MAE: 1.3980379104614258, Acc1: 0.5747663551401869, Acc2: 0.6119402985074627\n",
            "<------------ Saving Best Model\n",
            "\n",
            "start training ---------->>\n",
            "Train Epoch 14, total loss: 2.980074405670166, regression loss: 1.2677602767944336, embedding loss: 1.7123141288757324\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 14, MAE: 1.3945002555847168, Acc1: 0.5747663551401869, Acc2: 0.6119402985074627\n",
            "<------------ Saving Best Model\n",
            "\n",
            "start training ---------->>\n",
            "Train Epoch 15, total loss: 2.9411561489105225, regression loss: 1.2449206113815308, embedding loss: 1.6962355375289917\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 15, MAE: 1.3916347026824951, Acc1: 0.5747663551401869, Acc2: 0.6119402985074627\n",
            "<------------ Saving Best Model\n",
            "\n",
            "start training ---------->>\n",
            "Train Epoch 16, total loss: 2.910097599029541, regression loss: 1.1928095817565918, embedding loss: 1.7172880172729492\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 16, MAE: 1.3861583471298218, Acc1: 0.5747663551401869, Acc2: 0.6119402985074627\n",
            "<------------ Saving Best Model\n",
            "\n",
            "start training ---------->>\n",
            "Train Epoch 17, total loss: 2.8232717514038086, regression loss: 1.1027485132217407, embedding loss: 1.7205232381820679\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 17, MAE: 1.3647332191467285, Acc1: 0.5747663551401869, Acc2: 0.6119402985074627\n",
            "<------------ Saving Best Model\n",
            "\n",
            "start training ---------->>\n",
            "Train Epoch 18, total loss: 2.7461647987365723, regression loss: 1.0480490922927856, embedding loss: 1.6981157064437866\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 18, MAE: 1.3601881265640259, Acc1: 0.5654205607476636, Acc2: 0.6019900497512438\n",
            "<------------ Saving Best Model\n",
            "\n",
            "start training ---------->>\n",
            "Train Epoch 19, total loss: 2.7090907096862793, regression loss: 1.0001189708709717, embedding loss: 1.7089717388153076\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 19, MAE: 1.3651915788650513, Acc1: 0.6869158878504673, Acc2: 0.7014925373134329\n",
            "start training ---------->>\n",
            "Train Epoch 20, total loss: 2.6921539306640625, regression loss: 0.990617573261261, embedding loss: 1.7015364170074463\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 20, MAE: 1.3635547161102295, Acc1: 0.7009345794392523, Acc2: 0.7164179104477612\n",
            "start training ---------->>\n",
            "Train Epoch 21, total loss: 2.6999928951263428, regression loss: 0.9834326505661011, embedding loss: 1.7165602445602417\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 21, MAE: 1.3506922721862793, Acc1: 0.6682242990654206, Acc2: 0.6865671641791045\n",
            "<------------ Saving Best Model\n",
            "\n",
            "start training ---------->>\n",
            "Train Epoch 22, total loss: 2.686168670654297, regression loss: 0.9765722155570984, embedding loss: 1.7095963954925537\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 22, MAE: 1.3532623052597046, Acc1: 0.6121495327102804, Acc2: 0.6318407960199005\n",
            "start training ---------->>\n",
            "Train Epoch 23, total loss: 2.6687533855438232, regression loss: 0.9445654153823853, embedding loss: 1.724187970161438\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 23, MAE: 1.3419486284255981, Acc1: 0.6074766355140186, Acc2: 0.6318407960199005\n",
            "<------------ Saving Best Model\n",
            "\n",
            "start training ---------->>\n",
            "Train Epoch 24, total loss: 2.6516120433807373, regression loss: 0.9422523379325867, embedding loss: 1.7093596458435059\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 24, MAE: 1.3547582626342773, Acc1: 0.5934579439252337, Acc2: 0.6218905472636815\n",
            "start training ---------->>\n",
            "Train Epoch 25, total loss: 2.63714861869812, regression loss: 0.9278494119644165, embedding loss: 1.7092992067337036\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 25, MAE: 1.351528286933899, Acc1: 0.6915887850467289, Acc2: 0.7114427860696517\n",
            "start training ---------->>\n",
            "Train Epoch 26, total loss: 2.625706195831299, regression loss: 0.9119629859924316, embedding loss: 1.7137432098388672\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 26, MAE: 1.360978126525879, Acc1: 0.6869158878504673, Acc2: 0.7064676616915423\n",
            "start training ---------->>\n",
            "Train Epoch 27, total loss: 2.626899003982544, regression loss: 0.9163666367530823, embedding loss: 1.7105324268341064\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 27, MAE: 1.3577018976211548, Acc1: 0.5981308411214953, Acc2: 0.6268656716417911\n",
            "start training ---------->>\n",
            "Train Epoch 28, total loss: 2.616908073425293, regression loss: 0.9077865481376648, embedding loss: 1.7091214656829834\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 28, MAE: 1.3639706373214722, Acc1: 0.677570093457944, Acc2: 0.7014925373134329\n",
            "start training ---------->>\n",
            "Train Epoch 29, total loss: 2.590543031692505, regression loss: 0.8833612203598022, embedding loss: 1.7071818113327026\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 29, MAE: 1.3680649995803833, Acc1: 0.7383177570093458, Acc2: 0.7512437810945274\n",
            "start training ---------->>\n",
            "Train Epoch 30, total loss: 2.638012647628784, regression loss: 0.9435809850692749, embedding loss: 1.6944316625595093\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 30, MAE: 1.3699512481689453, Acc1: 0.7102803738317757, Acc2: 0.736318407960199\n",
            "start training ---------->>\n",
            "Train Epoch 31, total loss: 2.5761466026306152, regression loss: 0.8804839253425598, embedding loss: 1.6956627368927002\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 31, MAE: 1.3704214096069336, Acc1: 0.7476635514018691, Acc2: 0.746268656716418\n",
            "start training ---------->>\n",
            "Train Epoch 32, total loss: 2.6095521450042725, regression loss: 0.9042088389396667, embedding loss: 1.705343246459961\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 32, MAE: 1.3555169105529785, Acc1: 0.6355140186915887, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 33, total loss: 2.564835548400879, regression loss: 0.8598942160606384, embedding loss: 1.7049412727355957\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 33, MAE: 1.364760160446167, Acc1: 0.6962616822429907, Acc2: 0.7213930348258707\n",
            "start training ---------->>\n",
            "Train Epoch 34, total loss: 2.5906729698181152, regression loss: 0.8946328163146973, embedding loss: 1.696040153503418\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 34, MAE: 1.3550548553466797, Acc1: 0.6308411214953271, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 35, total loss: 2.5439634323120117, regression loss: 0.8429250717163086, embedding loss: 1.7010383605957031\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 35, MAE: 1.3520725965499878, Acc1: 0.6074766355140186, Acc2: 0.6417910447761194\n",
            "start training ---------->>\n",
            "Train Epoch 36, total loss: 2.579171895980835, regression loss: 0.8695047497749329, embedding loss: 1.7096672058105469\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 36, MAE: 1.351709008216858, Acc1: 0.6261682242990654, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 37, total loss: 2.573258876800537, regression loss: 0.8725979924201965, embedding loss: 1.7006609439849854\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 37, MAE: 1.3621259927749634, Acc1: 0.6074766355140186, Acc2: 0.6417910447761194\n",
            "start training ---------->>\n",
            "Train Epoch 38, total loss: 2.5382163524627686, regression loss: 0.8396658897399902, embedding loss: 1.6985504627227783\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 38, MAE: 1.356918215751648, Acc1: 0.5934579439252337, Acc2: 0.6268656716417911\n",
            "start training ---------->>\n",
            "Train Epoch 39, total loss: 2.5252254009246826, regression loss: 0.8265476226806641, embedding loss: 1.6986777782440186\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 39, MAE: 1.3426845073699951, Acc1: 0.6308411214953271, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 40, total loss: 2.540937662124634, regression loss: 0.837166428565979, embedding loss: 1.7037712335586548\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 40, MAE: 1.3667750358581543, Acc1: 0.6261682242990654, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 41, total loss: 2.592264413833618, regression loss: 0.8952881097793579, embedding loss: 1.6969763040542603\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 41, MAE: 1.3603142499923706, Acc1: 0.6962616822429907, Acc2: 0.7213930348258707\n",
            "start training ---------->>\n",
            "Train Epoch 42, total loss: 2.5465526580810547, regression loss: 0.8494155406951904, embedding loss: 1.6971371173858643\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 42, MAE: 1.3591099977493286, Acc1: 0.6355140186915887, Acc2: 0.6666666666666666\n",
            "start training ---------->>\n",
            "Train Epoch 43, total loss: 2.540243625640869, regression loss: 0.8261762857437134, embedding loss: 1.7140673398971558\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 43, MAE: 1.353573203086853, Acc1: 0.6962616822429907, Acc2: 0.7263681592039801\n",
            "start training ---------->>\n",
            "Train Epoch 44, total loss: 2.5884511470794678, regression loss: 0.8592128753662109, embedding loss: 1.7292382717132568\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 44, MAE: 1.3623945713043213, Acc1: 0.5981308411214953, Acc2: 0.6318407960199005\n",
            "start training ---------->>\n",
            "Train Epoch 45, total loss: 2.5066077709198, regression loss: 0.8041293025016785, embedding loss: 1.7024784088134766\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 45, MAE: 1.3465135097503662, Acc1: 0.6261682242990654, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 46, total loss: 2.524332284927368, regression loss: 0.822269856929779, embedding loss: 1.7020623683929443\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 46, MAE: 1.3521718978881836, Acc1: 0.6355140186915887, Acc2: 0.6666666666666666\n",
            "start training ---------->>\n",
            "Train Epoch 47, total loss: 2.554503917694092, regression loss: 0.8215790390968323, embedding loss: 1.7329249382019043\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 47, MAE: 1.3641959428787231, Acc1: 0.6448598130841121, Acc2: 0.6766169154228856\n",
            "start training ---------->>\n",
            "Train Epoch 48, total loss: 2.516265869140625, regression loss: 0.817040205001831, embedding loss: 1.699225664138794\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 48, MAE: 1.357316017150879, Acc1: 0.6261682242990654, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 49, total loss: 2.5044031143188477, regression loss: 0.8023348450660706, embedding loss: 1.7020683288574219\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 49, MAE: 1.3642845153808594, Acc1: 0.6401869158878505, Acc2: 0.6716417910447762\n",
            "start training ---------->>\n",
            "Train Epoch 50, total loss: 2.528841257095337, regression loss: 0.827053427696228, embedding loss: 1.7017878293991089\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 50, MAE: 1.3594372272491455, Acc1: 0.6869158878504673, Acc2: 0.7164179104477612\n",
            "start training ---------->>\n",
            "Train Epoch 51, total loss: 2.504647970199585, regression loss: 0.792944610118866, embedding loss: 1.7117033004760742\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 51, MAE: 1.3582298755645752, Acc1: 0.7009345794392523, Acc2: 0.7164179104477612\n",
            "start training ---------->>\n",
            "Train Epoch 52, total loss: 2.5206995010375977, regression loss: 0.8188320994377136, embedding loss: 1.7018673419952393\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 52, MAE: 1.361194372177124, Acc1: 0.6495327102803738, Acc2: 0.681592039800995\n",
            "start training ---------->>\n",
            "Train Epoch 53, total loss: 2.519329309463501, regression loss: 0.8032647967338562, embedding loss: 1.716064453125\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 53, MAE: 1.3588330745697021, Acc1: 0.6728971962616822, Acc2: 0.7064676616915423\n",
            "start training ---------->>\n",
            "Train Epoch 54, total loss: 2.477694272994995, regression loss: 0.7837678790092468, embedding loss: 1.6939263343811035\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 54, MAE: 1.3522217273712158, Acc1: 0.6355140186915887, Acc2: 0.6716417910447762\n",
            "start training ---------->>\n",
            "Train Epoch 55, total loss: 2.5165109634399414, regression loss: 0.8105931878089905, embedding loss: 1.7059178352355957\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 55, MAE: 1.3600164651870728, Acc1: 0.6869158878504673, Acc2: 0.7164179104477612\n",
            "start training ---------->>\n",
            "Train Epoch 56, total loss: 2.511080503463745, regression loss: 0.8073533773422241, embedding loss: 1.703727126121521\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 56, MAE: 1.3512160778045654, Acc1: 0.6448598130841121, Acc2: 0.6766169154228856\n",
            "start training ---------->>\n",
            "Train Epoch 57, total loss: 2.4916977882385254, regression loss: 0.791154682636261, embedding loss: 1.7005431652069092\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 57, MAE: 1.3628382682800293, Acc1: 0.6682242990654206, Acc2: 0.7014925373134329\n",
            "start training ---------->>\n",
            "Train Epoch 58, total loss: 2.5034680366516113, regression loss: 0.779764711856842, embedding loss: 1.723703384399414\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 58, MAE: 1.360538125038147, Acc1: 0.6495327102803738, Acc2: 0.681592039800995\n",
            "start training ---------->>\n",
            "Train Epoch 59, total loss: 2.492201089859009, regression loss: 0.7844763398170471, embedding loss: 1.7077248096466064\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 59, MAE: 1.3641294240951538, Acc1: 0.6448598130841121, Acc2: 0.6766169154228856\n",
            "start training ---------->>\n",
            "Train Epoch 60, total loss: 2.4649887084960938, regression loss: 0.7602955102920532, embedding loss: 1.7046931982040405\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 60, MAE: 1.3555676937103271, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 61, total loss: 2.508779764175415, regression loss: 0.8054044246673584, embedding loss: 1.7033753395080566\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 61, MAE: 1.3536759614944458, Acc1: 0.6308411214953271, Acc2: 0.6666666666666666\n",
            "start training ---------->>\n",
            "Train Epoch 62, total loss: 2.486118793487549, regression loss: 0.789555013179779, embedding loss: 1.696563720703125\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 62, MAE: 1.3596000671386719, Acc1: 0.6355140186915887, Acc2: 0.6716417910447762\n",
            "start training ---------->>\n",
            "Train Epoch 63, total loss: 2.4954426288604736, regression loss: 0.7898730635643005, embedding loss: 1.7055695056915283\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 63, MAE: 1.3652820587158203, Acc1: 0.6495327102803738, Acc2: 0.681592039800995\n",
            "start training ---------->>\n",
            "Train Epoch 64, total loss: 2.4653847217559814, regression loss: 0.7567647099494934, embedding loss: 1.7086200714111328\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 64, MAE: 1.3577104806900024, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 65, total loss: 2.472696542739868, regression loss: 0.7657768130302429, embedding loss: 1.7069196701049805\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 65, MAE: 1.356418490409851, Acc1: 0.602803738317757, Acc2: 0.6368159203980099\n",
            "start training ---------->>\n",
            "Train Epoch 66, total loss: 2.456965684890747, regression loss: 0.7437061667442322, embedding loss: 1.7132594585418701\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 66, MAE: 1.3674060106277466, Acc1: 0.5934579439252337, Acc2: 0.6268656716417911\n",
            "start training ---------->>\n",
            "Train Epoch 67, total loss: 2.448051929473877, regression loss: 0.7385628819465637, embedding loss: 1.709489107131958\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 67, MAE: 1.3656102418899536, Acc1: 0.677570093457944, Acc2: 0.7014925373134329\n",
            "start training ---------->>\n",
            "Train Epoch 68, total loss: 2.445331573486328, regression loss: 0.7337648272514343, embedding loss: 1.711566686630249\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 68, MAE: 1.3704240322113037, Acc1: 0.6635514018691588, Acc2: 0.6915422885572139\n",
            "start training ---------->>\n",
            "Train Epoch 69, total loss: 2.471695899963379, regression loss: 0.7701373100280762, embedding loss: 1.7015585899353027\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 69, MAE: 1.3706376552581787, Acc1: 0.6495327102803738, Acc2: 0.6865671641791045\n",
            "start training ---------->>\n",
            "Train Epoch 70, total loss: 2.456674814224243, regression loss: 0.7573617696762085, embedding loss: 1.6993130445480347\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 70, MAE: 1.3580355644226074, Acc1: 0.6074766355140186, Acc2: 0.6417910447761194\n",
            "start training ---------->>\n",
            "Train Epoch 71, total loss: 2.457606554031372, regression loss: 0.7478035688400269, embedding loss: 1.7098029851913452\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 71, MAE: 1.3631705045700073, Acc1: 0.6495327102803738, Acc2: 0.6865671641791045\n",
            "start training ---------->>\n",
            "Train Epoch 72, total loss: 2.4546611309051514, regression loss: 0.7498039603233337, embedding loss: 1.7048571109771729\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 72, MAE: 1.3557671308517456, Acc1: 0.6542056074766355, Acc2: 0.6915422885572139\n",
            "start training ---------->>\n",
            "Train Epoch 73, total loss: 2.464810371398926, regression loss: 0.7413755655288696, embedding loss: 1.7234348058700562\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 73, MAE: 1.3649158477783203, Acc1: 0.6121495327102804, Acc2: 0.6467661691542289\n",
            "start training ---------->>\n",
            "Train Epoch 74, total loss: 2.4182348251342773, regression loss: 0.7130029201507568, embedding loss: 1.7052319049835205\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 74, MAE: 1.366227149963379, Acc1: 0.6121495327102804, Acc2: 0.6467661691542289\n",
            "start training ---------->>\n",
            "Train Epoch 75, total loss: 2.4723052978515625, regression loss: 0.742779016494751, embedding loss: 1.7295262813568115\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 75, MAE: 1.3656970262527466, Acc1: 0.6495327102803738, Acc2: 0.681592039800995\n",
            "start training ---------->>\n",
            "Train Epoch 76, total loss: 2.439516067504883, regression loss: 0.7437359690666199, embedding loss: 1.6957800388336182\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 76, MAE: 1.3634897470474243, Acc1: 0.6448598130841121, Acc2: 0.681592039800995\n",
            "start training ---------->>\n",
            "Train Epoch 77, total loss: 2.431495428085327, regression loss: 0.7294737100601196, embedding loss: 1.7020217180252075\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 77, MAE: 1.3622078895568848, Acc1: 0.6448598130841121, Acc2: 0.6766169154228856\n",
            "start training ---------->>\n",
            "Train Epoch 78, total loss: 2.4100847244262695, regression loss: 0.7139016389846802, embedding loss: 1.6961830854415894\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 78, MAE: 1.3692638874053955, Acc1: 0.6495327102803738, Acc2: 0.6865671641791045\n",
            "start training ---------->>\n",
            "Train Epoch 79, total loss: 2.406690835952759, regression loss: 0.7080087065696716, embedding loss: 1.6986820697784424\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 79, MAE: 1.3653730154037476, Acc1: 0.6495327102803738, Acc2: 0.6766169154228856\n",
            "start training ---------->>\n",
            "Train Epoch 80, total loss: 2.4045801162719727, regression loss: 0.7083042860031128, embedding loss: 1.6962758302688599\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 80, MAE: 1.3666419982910156, Acc1: 0.6074766355140186, Acc2: 0.6417910447761194\n",
            "start training ---------->>\n",
            "Train Epoch 81, total loss: 2.415269613265991, regression loss: 0.7223264575004578, embedding loss: 1.6929430961608887\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 81, MAE: 1.366141438484192, Acc1: 0.6495327102803738, Acc2: 0.681592039800995\n",
            "start training ---------->>\n",
            "Train Epoch 82, total loss: 2.4467151165008545, regression loss: 0.7306899428367615, embedding loss: 1.7160251140594482\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 82, MAE: 1.364707350730896, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 83, total loss: 2.39213228225708, regression loss: 0.6936700940132141, embedding loss: 1.6984622478485107\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 83, MAE: 1.3670845031738281, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 84, total loss: 2.4277920722961426, regression loss: 0.7289020419120789, embedding loss: 1.698889970779419\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 84, MAE: 1.366290807723999, Acc1: 0.6401869158878505, Acc2: 0.6716417910447762\n",
            "start training ---------->>\n",
            "Train Epoch 85, total loss: 2.406588315963745, regression loss: 0.6994505524635315, embedding loss: 1.7071378231048584\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 85, MAE: 1.363509178161621, Acc1: 0.6448598130841121, Acc2: 0.6766169154228856\n",
            "start training ---------->>\n",
            "Train Epoch 86, total loss: 2.4160096645355225, regression loss: 0.7093161940574646, embedding loss: 1.706693410873413\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 86, MAE: 1.3691964149475098, Acc1: 0.6355140186915887, Acc2: 0.6666666666666666\n",
            "start training ---------->>\n",
            "Train Epoch 87, total loss: 2.409445285797119, regression loss: 0.6996129155158997, embedding loss: 1.7098324298858643\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 87, MAE: 1.364173412322998, Acc1: 0.6495327102803738, Acc2: 0.681592039800995\n",
            "start training ---------->>\n",
            "Train Epoch 88, total loss: 2.4481658935546875, regression loss: 0.7426315546035767, embedding loss: 1.7055343389511108\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 88, MAE: 1.3677732944488525, Acc1: 0.6448598130841121, Acc2: 0.6766169154228856\n",
            "start training ---------->>\n",
            "Train Epoch 89, total loss: 2.404184103012085, regression loss: 0.7079522609710693, embedding loss: 1.6962318420410156\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 89, MAE: 1.3663976192474365, Acc1: 0.6355140186915887, Acc2: 0.6666666666666666\n",
            "start training ---------->>\n",
            "Train Epoch 90, total loss: 2.3992629051208496, regression loss: 0.6912969946861267, embedding loss: 1.7079658508300781\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 90, MAE: 1.3640309572219849, Acc1: 0.6588785046728972, Acc2: 0.681592039800995\n",
            "start training ---------->>\n",
            "Train Epoch 91, total loss: 2.396392583847046, regression loss: 0.6746119856834412, embedding loss: 1.72178053855896\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 91, MAE: 1.37245512008667, Acc1: 0.6682242990654206, Acc2: 0.6915422885572139\n",
            "start training ---------->>\n",
            "Train Epoch 92, total loss: 2.39872145652771, regression loss: 0.6918378472328186, embedding loss: 1.7068836688995361\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 92, MAE: 1.366172432899475, Acc1: 0.6308411214953271, Acc2: 0.6666666666666666\n",
            "start training ---------->>\n",
            "Train Epoch 93, total loss: 2.4034364223480225, regression loss: 0.7065090537071228, embedding loss: 1.6969273090362549\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 93, MAE: 1.364607810974121, Acc1: 0.6542056074766355, Acc2: 0.6865671641791045\n",
            "start training ---------->>\n",
            "Train Epoch 94, total loss: 2.4001660346984863, regression loss: 0.6865806579589844, embedding loss: 1.713585376739502\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 94, MAE: 1.3730448484420776, Acc1: 0.6448598130841121, Acc2: 0.6766169154228856\n",
            "start training ---------->>\n",
            "Train Epoch 95, total loss: 2.4337239265441895, regression loss: 0.7198580503463745, embedding loss: 1.713865876197815\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 95, MAE: 1.3727052211761475, Acc1: 0.6308411214953271, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 96, total loss: 2.3874096870422363, regression loss: 0.6800397634506226, embedding loss: 1.7073699235916138\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 96, MAE: 1.3596205711364746, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 97, total loss: 2.390146493911743, regression loss: 0.6841909885406494, embedding loss: 1.7059555053710938\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 97, MAE: 1.3663692474365234, Acc1: 0.6308411214953271, Acc2: 0.6666666666666666\n",
            "start training ---------->>\n",
            "Train Epoch 98, total loss: 2.3835771083831787, regression loss: 0.6804147362709045, embedding loss: 1.703162431716919\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 98, MAE: 1.3642042875289917, Acc1: 0.6308411214953271, Acc2: 0.6666666666666666\n",
            "start training ---------->>\n",
            "Train Epoch 99, total loss: 2.3869118690490723, regression loss: 0.6726981401443481, embedding loss: 1.7142137289047241\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 99, MAE: 1.3681137561798096, Acc1: 0.6448598130841121, Acc2: 0.6766169154228856\n",
            "start training ---------->>\n",
            "Train Epoch 100, total loss: 2.425198793411255, regression loss: 0.715725302696228, embedding loss: 1.7094734907150269\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 100, MAE: 1.3662474155426025, Acc1: 0.6121495327102804, Acc2: 0.6467661691542289\n",
            "start training ---------->>\n",
            "Train Epoch 101, total loss: 2.38608980178833, regression loss: 0.6722213625907898, embedding loss: 1.7138683795928955\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 101, MAE: 1.3615964651107788, Acc1: 0.6401869158878505, Acc2: 0.6716417910447762\n",
            "start training ---------->>\n",
            "Train Epoch 102, total loss: 2.3645122051239014, regression loss: 0.672613799571991, embedding loss: 1.6918983459472656\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 102, MAE: 1.3687082529067993, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 103, total loss: 2.3536713123321533, regression loss: 0.6614061594009399, embedding loss: 1.6922651529312134\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 103, MAE: 1.367964744567871, Acc1: 0.6542056074766355, Acc2: 0.681592039800995\n",
            "start training ---------->>\n",
            "Train Epoch 104, total loss: 2.3772382736206055, regression loss: 0.6694114208221436, embedding loss: 1.707826852798462\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 104, MAE: 1.3637627363204956, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 105, total loss: 2.398040771484375, regression loss: 0.6814805865287781, embedding loss: 1.7165601253509521\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 105, MAE: 1.366533637046814, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 106, total loss: 2.3656973838806152, regression loss: 0.652015209197998, embedding loss: 1.7136821746826172\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 106, MAE: 1.3676426410675049, Acc1: 0.6121495327102804, Acc2: 0.6467661691542289\n",
            "start training ---------->>\n",
            "Train Epoch 107, total loss: 2.3831846714019775, regression loss: 0.673746645450592, embedding loss: 1.7094380855560303\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 107, MAE: 1.361401081085205, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 108, total loss: 2.359172821044922, regression loss: 0.6624689102172852, embedding loss: 1.6967039108276367\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 108, MAE: 1.3703862428665161, Acc1: 0.6261682242990654, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 109, total loss: 2.328446388244629, regression loss: 0.628843367099762, embedding loss: 1.6996030807495117\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 109, MAE: 1.371852993965149, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 110, total loss: 2.321101665496826, regression loss: 0.622079074382782, embedding loss: 1.6990225315093994\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 110, MAE: 1.3739951848983765, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 111, total loss: 2.349672794342041, regression loss: 0.6511942148208618, embedding loss: 1.6984785795211792\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 111, MAE: 1.3657475709915161, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 112, total loss: 2.3280444145202637, regression loss: 0.630216658115387, embedding loss: 1.6978278160095215\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 112, MAE: 1.3696868419647217, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 113, total loss: 2.3652920722961426, regression loss: 0.6536091566085815, embedding loss: 1.711682915687561\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 113, MAE: 1.376003384590149, Acc1: 0.6214953271028038, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 114, total loss: 2.321333169937134, regression loss: 0.6171479225158691, embedding loss: 1.7041852474212646\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 114, MAE: 1.3687654733657837, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 115, total loss: 2.3613061904907227, regression loss: 0.6608951091766357, embedding loss: 1.700411081314087\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 115, MAE: 1.366207242012024, Acc1: 0.6308411214953271, Acc2: 0.6666666666666666\n",
            "start training ---------->>\n",
            "Train Epoch 116, total loss: 2.394892454147339, regression loss: 0.6935869455337524, embedding loss: 1.7013055086135864\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 116, MAE: 1.3674395084381104, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 117, total loss: 2.3672380447387695, regression loss: 0.6482473015785217, embedding loss: 1.7189908027648926\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 117, MAE: 1.3712972402572632, Acc1: 0.6308411214953271, Acc2: 0.6666666666666666\n",
            "start training ---------->>\n",
            "Train Epoch 118, total loss: 2.370438814163208, regression loss: 0.6757771968841553, embedding loss: 1.6946616172790527\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 118, MAE: 1.3634028434753418, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 119, total loss: 2.3307931423187256, regression loss: 0.627581000328064, embedding loss: 1.7032121419906616\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 119, MAE: 1.3625009059906006, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 120, total loss: 2.3504638671875, regression loss: 0.6492360234260559, embedding loss: 1.7012279033660889\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 120, MAE: 1.37159264087677, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 121, total loss: 2.3141767978668213, regression loss: 0.6002346277236938, embedding loss: 1.7139421701431274\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 121, MAE: 1.367648959159851, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 122, total loss: 2.3387317657470703, regression loss: 0.6348913311958313, embedding loss: 1.7038404941558838\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 122, MAE: 1.3641483783721924, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 123, total loss: 2.3561699390411377, regression loss: 0.640587329864502, embedding loss: 1.7155826091766357\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 123, MAE: 1.3669826984405518, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 124, total loss: 2.3714945316314697, regression loss: 0.6577017307281494, embedding loss: 1.7137928009033203\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 124, MAE: 1.3749885559082031, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 125, total loss: 2.3277595043182373, regression loss: 0.6184730529785156, embedding loss: 1.7092864513397217\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 125, MAE: 1.3713101148605347, Acc1: 0.6121495327102804, Acc2: 0.6467661691542289\n",
            "start training ---------->>\n",
            "Train Epoch 126, total loss: 2.364440679550171, regression loss: 0.6466750502586365, embedding loss: 1.7177655696868896\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 126, MAE: 1.3733423948287964, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 127, total loss: 2.313249349594116, regression loss: 0.6102112531661987, embedding loss: 1.7030380964279175\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 127, MAE: 1.3740633726119995, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 128, total loss: 2.342648983001709, regression loss: 0.6197095513343811, embedding loss: 1.7229394912719727\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 128, MAE: 1.3706765174865723, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 129, total loss: 2.317918539047241, regression loss: 0.6143896579742432, embedding loss: 1.703528881072998\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 129, MAE: 1.3677995204925537, Acc1: 0.6308411214953271, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 130, total loss: 2.3119678497314453, regression loss: 0.6129050254821777, embedding loss: 1.6990628242492676\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 130, MAE: 1.3678321838378906, Acc1: 0.6308411214953271, Acc2: 0.6666666666666666\n",
            "start training ---------->>\n",
            "Train Epoch 131, total loss: 2.3354856967926025, regression loss: 0.6326403021812439, embedding loss: 1.7028453350067139\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 131, MAE: 1.3745577335357666, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 132, total loss: 2.3266563415527344, regression loss: 0.6282831430435181, embedding loss: 1.6983731985092163\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 132, MAE: 1.371601939201355, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 133, total loss: 2.3119897842407227, regression loss: 0.6193650364875793, embedding loss: 1.692624807357788\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 133, MAE: 1.368922472000122, Acc1: 0.6728971962616822, Acc2: 0.7064676616915423\n",
            "start training ---------->>\n",
            "Train Epoch 134, total loss: 2.3568294048309326, regression loss: 0.6488492488861084, embedding loss: 1.7079801559448242\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 134, MAE: 1.3735167980194092, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 135, total loss: 2.3342649936676025, regression loss: 0.631675124168396, embedding loss: 1.7025898694992065\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 135, MAE: 1.3684353828430176, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 136, total loss: 2.307722330093384, regression loss: 0.6015996932983398, embedding loss: 1.706122636795044\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 136, MAE: 1.3716903924942017, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 137, total loss: 2.313865900039673, regression loss: 0.6005130410194397, embedding loss: 1.713352918624878\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 137, MAE: 1.3730095624923706, Acc1: 0.6308411214953271, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 138, total loss: 2.301126480102539, regression loss: 0.6019436717033386, embedding loss: 1.6991827487945557\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 138, MAE: 1.3719511032104492, Acc1: 0.6214953271028038, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 139, total loss: 2.2665581703186035, regression loss: 0.5622677803039551, embedding loss: 1.7042903900146484\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 139, MAE: 1.3714083433151245, Acc1: 0.6495327102803738, Acc2: 0.681592039800995\n",
            "start training ---------->>\n",
            "Train Epoch 140, total loss: 2.3152689933776855, regression loss: 0.6107283234596252, embedding loss: 1.704540729522705\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 140, MAE: 1.3713139295578003, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 141, total loss: 2.2999980449676514, regression loss: 0.6021588444709778, embedding loss: 1.6978392601013184\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 141, MAE: 1.3727613687515259, Acc1: 0.6121495327102804, Acc2: 0.6467661691542289\n",
            "start training ---------->>\n",
            "Train Epoch 142, total loss: 2.2927489280700684, regression loss: 0.5899159908294678, embedding loss: 1.7028329372406006\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 142, MAE: 1.37195885181427, Acc1: 0.6308411214953271, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 143, total loss: 2.2886061668395996, regression loss: 0.5733059644699097, embedding loss: 1.71530020236969\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 143, MAE: 1.3624268770217896, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 144, total loss: 2.2966606616973877, regression loss: 0.5969512462615967, embedding loss: 1.699709415435791\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 144, MAE: 1.3640402555465698, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 145, total loss: 2.307365894317627, regression loss: 0.603786289691925, embedding loss: 1.7035796642303467\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 145, MAE: 1.3693667650222778, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 146, total loss: 2.270841598510742, regression loss: 0.5697345733642578, embedding loss: 1.7011070251464844\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 146, MAE: 1.3678358793258667, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 147, total loss: 2.2927324771881104, regression loss: 0.584318220615387, embedding loss: 1.7084143161773682\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 147, MAE: 1.3740653991699219, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 148, total loss: 2.2841880321502686, regression loss: 0.5882259011268616, embedding loss: 1.6959621906280518\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 148, MAE: 1.3668632507324219, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 149, total loss: 2.274451732635498, regression loss: 0.5757954120635986, embedding loss: 1.6986563205718994\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 149, MAE: 1.3676609992980957, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 150, total loss: 2.30956768989563, regression loss: 0.5953646302223206, embedding loss: 1.714203119277954\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 150, MAE: 1.3647347688674927, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 151, total loss: 2.271857500076294, regression loss: 0.5697356462478638, embedding loss: 1.7021218538284302\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 151, MAE: 1.3687790632247925, Acc1: 0.6121495327102804, Acc2: 0.6467661691542289\n",
            "start training ---------->>\n",
            "Train Epoch 152, total loss: 2.2496707439422607, regression loss: 0.554661214351654, embedding loss: 1.695009469985962\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 152, MAE: 1.374027967453003, Acc1: 0.6682242990654206, Acc2: 0.6965174129353234\n",
            "start training ---------->>\n",
            "Train Epoch 153, total loss: 2.292670965194702, regression loss: 0.5921370387077332, embedding loss: 1.7005338668823242\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 153, MAE: 1.365097165107727, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 154, total loss: 2.307512044906616, regression loss: 0.5977526903152466, embedding loss: 1.7097593545913696\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 154, MAE: 1.361484169960022, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 155, total loss: 2.309379816055298, regression loss: 0.6129952073097229, embedding loss: 1.6963846683502197\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 155, MAE: 1.3647249937057495, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 156, total loss: 2.3008697032928467, regression loss: 0.6017531752586365, embedding loss: 1.6991164684295654\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 156, MAE: 1.3704668283462524, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 157, total loss: 2.2591657638549805, regression loss: 0.5604594945907593, embedding loss: 1.6987062692642212\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 157, MAE: 1.367008090019226, Acc1: 0.6121495327102804, Acc2: 0.6467661691542289\n",
            "start training ---------->>\n",
            "Train Epoch 158, total loss: 2.2655229568481445, regression loss: 0.5498875975608826, embedding loss: 1.7156352996826172\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 158, MAE: 1.3642514944076538, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 159, total loss: 2.281682014465332, regression loss: 0.5819711089134216, embedding loss: 1.6997108459472656\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 159, MAE: 1.3721940517425537, Acc1: 0.6308411214953271, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 160, total loss: 2.286757707595825, regression loss: 0.57366943359375, embedding loss: 1.7130882740020752\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 160, MAE: 1.3654857873916626, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 161, total loss: 2.274707317352295, regression loss: 0.5790969133377075, embedding loss: 1.6956104040145874\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 161, MAE: 1.366978406906128, Acc1: 0.6355140186915887, Acc2: 0.6666666666666666\n",
            "start training ---------->>\n",
            "Train Epoch 162, total loss: 2.2706332206726074, regression loss: 0.5703044533729553, embedding loss: 1.7003288269042969\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 162, MAE: 1.364115834236145, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 163, total loss: 2.268509864807129, regression loss: 0.5570709705352783, embedding loss: 1.7114388942718506\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 163, MAE: 1.3669764995574951, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 164, total loss: 2.286585569381714, regression loss: 0.5868918299674988, embedding loss: 1.6996936798095703\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 164, MAE: 1.3625141382217407, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 165, total loss: 2.2615182399749756, regression loss: 0.5652874112129211, embedding loss: 1.6962308883666992\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 165, MAE: 1.3635685443878174, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 166, total loss: 2.266260862350464, regression loss: 0.5551465749740601, embedding loss: 1.7111142873764038\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 166, MAE: 1.364748239517212, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 167, total loss: 2.2972464561462402, regression loss: 0.5941281914710999, embedding loss: 1.7031183242797852\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 167, MAE: 1.3666199445724487, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 168, total loss: 2.2690320014953613, regression loss: 0.5481128692626953, embedding loss: 1.720919132232666\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 168, MAE: 1.3695600032806396, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 169, total loss: 2.2477476596832275, regression loss: 0.5477877855300903, embedding loss: 1.6999598741531372\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 169, MAE: 1.3653724193572998, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 170, total loss: 2.262209177017212, regression loss: 0.5547942519187927, embedding loss: 1.7074148654937744\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 170, MAE: 1.3684742450714111, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 171, total loss: 2.249018430709839, regression loss: 0.5483736395835876, embedding loss: 1.7006447315216064\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 171, MAE: 1.3567832708358765, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 172, total loss: 2.2784571647644043, regression loss: 0.5720114707946777, embedding loss: 1.7064456939697266\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 172, MAE: 1.3727545738220215, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 173, total loss: 2.310601234436035, regression loss: 0.5899114012718201, embedding loss: 1.7206897735595703\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 173, MAE: 1.3655872344970703, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 174, total loss: 2.2329375743865967, regression loss: 0.5331620573997498, embedding loss: 1.6997754573822021\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 174, MAE: 1.3665838241577148, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 175, total loss: 2.2454893589019775, regression loss: 0.541350245475769, embedding loss: 1.7041391134262085\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 175, MAE: 1.3641026020050049, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 176, total loss: 2.303258180618286, regression loss: 0.5908787846565247, embedding loss: 1.7123794555664062\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 176, MAE: 1.3630262613296509, Acc1: 0.6355140186915887, Acc2: 0.6716417910447762\n",
            "start training ---------->>\n",
            "Train Epoch 177, total loss: 2.2775163650512695, regression loss: 0.5587370991706848, embedding loss: 1.7187793254852295\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 177, MAE: 1.3628451824188232, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 178, total loss: 2.267357110977173, regression loss: 0.5596693158149719, embedding loss: 1.7076878547668457\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 178, MAE: 1.362781047821045, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 179, total loss: 2.186255931854248, regression loss: 0.49353086948394775, embedding loss: 1.6927250623703003\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 179, MAE: 1.3633766174316406, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 180, total loss: 2.2731640338897705, regression loss: 0.5654320120811462, embedding loss: 1.7077319622039795\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 180, MAE: 1.3701132535934448, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 181, total loss: 2.259155511856079, regression loss: 0.5584046840667725, embedding loss: 1.7007508277893066\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 181, MAE: 1.3707953691482544, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 182, total loss: 2.26462721824646, regression loss: 0.5538188815116882, embedding loss: 1.710808277130127\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 182, MAE: 1.369918942451477, Acc1: 0.6074766355140186, Acc2: 0.6417910447761194\n",
            "start training ---------->>\n",
            "Train Epoch 183, total loss: 2.2429654598236084, regression loss: 0.5430353283882141, embedding loss: 1.699930191040039\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 183, MAE: 1.365254282951355, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 184, total loss: 2.2678678035736084, regression loss: 0.5595247149467468, embedding loss: 1.7083430290222168\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 184, MAE: 1.364784836769104, Acc1: 0.6308411214953271, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 185, total loss: 2.271381378173828, regression loss: 0.5679774284362793, embedding loss: 1.7034039497375488\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 185, MAE: 1.361080527305603, Acc1: 0.6308411214953271, Acc2: 0.6666666666666666\n",
            "start training ---------->>\n",
            "Train Epoch 186, total loss: 2.262544631958008, regression loss: 0.5553648471832275, embedding loss: 1.7071797847747803\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 186, MAE: 1.3602360486984253, Acc1: 0.5981308411214953, Acc2: 0.6318407960199005\n",
            "start training ---------->>\n",
            "Train Epoch 187, total loss: 2.246176242828369, regression loss: 0.5455340147018433, embedding loss: 1.7006422281265259\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 187, MAE: 1.3648457527160645, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 188, total loss: 2.257402181625366, regression loss: 0.5442768931388855, embedding loss: 1.713125228881836\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 188, MAE: 1.3650164604187012, Acc1: 0.6355140186915887, Acc2: 0.6716417910447762\n",
            "start training ---------->>\n",
            "Train Epoch 189, total loss: 2.2518718242645264, regression loss: 0.5345857739448547, embedding loss: 1.7172861099243164\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 189, MAE: 1.3619611263275146, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 190, total loss: 2.2561588287353516, regression loss: 0.5582700371742249, embedding loss: 1.6978888511657715\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 190, MAE: 1.3643044233322144, Acc1: 0.6355140186915887, Acc2: 0.6716417910447762\n",
            "start training ---------->>\n",
            "Train Epoch 191, total loss: 2.2503790855407715, regression loss: 0.5328728556632996, embedding loss: 1.7175061702728271\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 191, MAE: 1.3644839525222778, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 192, total loss: 2.2657835483551025, regression loss: 0.5463549494743347, embedding loss: 1.719428539276123\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 192, MAE: 1.3678240776062012, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 193, total loss: 2.2656798362731934, regression loss: 0.5612610578536987, embedding loss: 1.7044187784194946\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 193, MAE: 1.3643901348114014, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 194, total loss: 2.2705142498016357, regression loss: 0.5631197094917297, embedding loss: 1.7073945999145508\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 194, MAE: 1.3649214506149292, Acc1: 0.616822429906542, Acc2: 0.6517412935323383\n",
            "start training ---------->>\n",
            "Train Epoch 195, total loss: 2.219468593597412, regression loss: 0.5111865401268005, embedding loss: 1.7082819938659668\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 195, MAE: 1.3640815019607544, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 196, total loss: 2.237854480743408, regression loss: 0.519951343536377, embedding loss: 1.7179031372070312\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 196, MAE: 1.3713372945785522, Acc1: 0.6214953271028038, Acc2: 0.6567164179104478\n",
            "start training ---------->>\n",
            "Train Epoch 197, total loss: 2.2365057468414307, regression loss: 0.5340383052825928, embedding loss: 1.702467441558838\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 197, MAE: 1.363368272781372, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 198, total loss: 2.2334463596343994, regression loss: 0.5328028202056885, embedding loss: 1.700643539428711\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 198, MAE: 1.3668462038040161, Acc1: 0.6308411214953271, Acc2: 0.6616915422885572\n",
            "start training ---------->>\n",
            "Train Epoch 199, total loss: 2.2083709239959717, regression loss: 0.5094588398933411, embedding loss: 1.6989121437072754\n",
            "Start Evaluating ---------->>\n",
            "Eval Epoch: 199, MAE: 1.3639121055603027, Acc1: 0.6261682242990654, Acc2: 0.6616915422885572\n",
            "Training Time: 1085.8706154823303\n",
            "Training Peak Mem: 5861.78125\n",
            "Training Params: 198809\n",
            "Start Testing ---------->>\n",
            "Test: MAE: 1.3914631605148315, Acc1: 0.5043731778425656, Acc2: 0.5213414634146342\n",
            "Inference Time: 0.6300766468048096\n",
            "Inference Params: 198809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's import the dataloaders for MOSI, and import that data using the path we stored the MOSI_RAW.pkl file to."
      ],
      "metadata": {
        "id": "0gfx9D-nXqIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets.affect.get_data import get_dataloader # noqa\n",
        "\n",
        "\n",
        "traindata, validdata, testdata = \\\n",
        "    get_dataloader('/content/MultiBench/mosi_raw.pkl', robust_test=False)"
      ],
      "metadata": {
        "id": "6DvpItXYXqjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's define the encoder and decoder modules for each modality, taken from the associated section of MultiBench."
      ],
      "metadata": {
        "id": "ggW11eOjX4n1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unimodals.common_models import GRU, MLP # noqa\n",
        "from fusions.MCTN import Encoder, Decoder # noqa\n",
        "\n",
        "\n",
        "max_seq = 20\n",
        "feature_dim = 300\n",
        "hidden_dim = 32\n",
        "\n",
        "encoder0 = Encoder(feature_dim, hidden_dim, n_layers=1, dropout=0.0).cuda()\n",
        "decoder0 = Decoder(hidden_dim, feature_dim, n_layers=1, dropout=0.0).cuda()\n",
        "encoder1 = Encoder(hidden_dim, hidden_dim, n_layers=1, dropout=0.0).cuda()\n",
        "decoder1 = Decoder(hidden_dim, feature_dim, n_layers=1, dropout=0.0).cuda()\n",
        "\n",
        "reg_encoder = nn.GRU(hidden_dim, 32).cuda()"
      ],
      "metadata": {
        "id": "xOspjX4baCMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's define the classification head for our model:"
      ],
      "metadata": {
        "id": "IsU9RQ7NaTMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unimodals.common_models import MLP # noqa\n",
        "head = MLP(32, 64, 1).cuda()"
      ],
      "metadata": {
        "id": "9Uf1YBOJaSgR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, let's define the model training structure, and train our MCTN network. Here, it takes in not only encoders and decoders for each input modality, but also the reg_encoder:"
      ],
      "metadata": {
        "id": "-eqrIMdGaEW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from private_test_scripts.all_in_one import all_in_one_train # noqa\n",
        "from training_structures.MCTN_Level2 import train, test # noqa\n",
        "\n",
        "allmodules = [encoder0, decoder0, encoder1, decoder1, reg_encoder, head]\n",
        "\n",
        "\n",
        "def trainprocess():\n",
        "    train(\n",
        "        traindata, validdata,\n",
        "        encoder0, decoder0, encoder1, decoder1,\n",
        "        reg_encoder, head,\n",
        "        criterion_t0=nn.MSELoss(), criterion_c=nn.MSELoss(),\n",
        "        criterion_t1=nn.MSELoss(), criterion_r=nn.L1Loss(),\n",
        "        max_seq_len=20,\n",
        "        mu_t0=0.01, mu_c=0.01, mu_t1=0.01,\n",
        "        dropout_p=0.15, early_stop=False, patience_num=15,\n",
        "        lr=1e-4, weight_decay=0.01, op_type=torch.optim.AdamW,\n",
        "        epoch=200, model_save='best_mctn.pt')\n",
        "\n",
        "\n",
        "all_in_one_train(trainprocess, allmodules)\n",
        "\n",
        "model = torch.load('best_mctn.pt').cuda()\n",
        "\n",
        "test(model, testdata, 'mosi', no_robust=True)\n"
      ],
      "metadata": {
        "id": "IPVgwviCX5ik"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}